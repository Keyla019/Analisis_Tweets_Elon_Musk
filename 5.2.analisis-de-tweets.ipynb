{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "41154291-76ee-416d-8106-9d47c52b272f",
   "metadata": {
    "id": "41154291-76ee-416d-8106-9d47c52b272f"
   },
   "source": [
    "# Análisis de tweets\n",
    "\n",
    "*Elon Musk*\n",
    "\n",
    "Se facilita una lista de `dict` con datos reales de ventas con las siguientes keys:\n",
    "\n",
    "* 'Unnamed: 0'\n",
    "* 'id'\n",
    "* 'conversation_id'\n",
    "* 'created_at'\n",
    "* 'date'\n",
    "* 'timezone'\n",
    "* 'place'\n",
    "* 'tweet'\n",
    "* 'language'\n",
    "* 'hashtags'\n",
    "* 'cashtags'\n",
    "* 'user_id'\n",
    "* 'user_id_str'\n",
    "* 'username'\n",
    "* 'name'\n",
    "* 'day'\n",
    "* 'hour'\n",
    "* 'link'\n",
    "* 'urls'\n",
    "* 'photos'\n",
    "* 'video'\n",
    "* 'thumbnail'\n",
    "* 'retweet'\n",
    "* 'nlikes'\n",
    "* 'nreplies'\n",
    "* 'nretweets'\n",
    "* 'quote_url'\n",
    "* 'search'\n",
    "* 'near'\n",
    "* 'geo'\n",
    "* 'source'\n",
    "* 'user_rt_id'\n",
    "* 'user_rt'\n",
    "* 'retweet_id'\n",
    "* 'reply_to'\n",
    "* 'retweet_date'\n",
    "* 'translate'\n",
    "* 'trans_src'\n",
    "* 'trans_dest'\n",
    "\n",
    "y values de diferentes tipos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b5d05795-bd04-427e-b8f4-004b9b52f160",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "import collections\n",
    "\n",
    "with open('elon_tweets.pickle', 'rb') as handle:\n",
    "    elon_tweets = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a798c33a-c2b0-4cb6-b55d-247897f35815",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'Unnamed: 0': 0,\n",
       "  'id': 15434727182,\n",
       "  'conversation_id': 15434727182,\n",
       "  'created_at': 1275676317000.0,\n",
       "  'date': '2010-06-04 18:31:57',\n",
       "  'timezone': 0,\n",
       "  'place': nan,\n",
       "  'tweet': 'Please ignore prior tweets, as that was someone pretending to be me :)  This is actually me.',\n",
       "  'language': 'en',\n",
       "  'hashtags': '[]',\n",
       "  'cashtags': '[]',\n",
       "  'user_id': 44196397,\n",
       "  'user_id_str': 44196397,\n",
       "  'username': 'elonmusk',\n",
       "  'name': 'Elon Musk',\n",
       "  'day': 5,\n",
       "  'hour': 18,\n",
       "  'link': 'https://twitter.com/elonmusk/status/15434727182',\n",
       "  'urls': '[]',\n",
       "  'photos': '[]',\n",
       "  'video': 0,\n",
       "  'thumbnail': nan,\n",
       "  'retweet': False,\n",
       "  'nlikes': 4652,\n",
       "  'nreplies': 391,\n",
       "  'nretweets': 348,\n",
       "  'quote_url': nan,\n",
       "  'search': 'None',\n",
       "  'near': nan,\n",
       "  'geo': nan,\n",
       "  'source': nan,\n",
       "  'user_rt_id': nan,\n",
       "  'user_rt': nan,\n",
       "  'retweet_id': nan,\n",
       "  'reply_to': '[]',\n",
       "  'retweet_date': nan,\n",
       "  'translate': nan,\n",
       "  'trans_src': nan,\n",
       "  'trans_dest': nan},\n",
       " {'Unnamed: 0': 0,\n",
       "  'id': 679163784062107649,\n",
       "  'conversation_id': 679163784062107649,\n",
       "  'created_at': 1450760239000.0,\n",
       "  'date': '2015-12-22 04:57:19',\n",
       "  'timezone': 0,\n",
       "  'place': nan,\n",
       "  'tweet': 'High res video of landing from the helo  https://t.co/G0Yq2V5J3m',\n",
       "  'language': 'en',\n",
       "  'hashtags': '[]',\n",
       "  'cashtags': '[]',\n",
       "  'user_id': 44196397,\n",
       "  'user_id_str': 44196397,\n",
       "  'username': 'elonmusk',\n",
       "  'name': 'Elon Musk',\n",
       "  'day': 2,\n",
       "  'hour': 4,\n",
       "  'link': 'https://twitter.com/elonmusk/status/679163784062107649',\n",
       "  'urls': \"['https://m.youtube.com/watch?v=ZCBE8ocOkAQ&feature=youtu.be']\",\n",
       "  'photos': '[]',\n",
       "  'video': 0,\n",
       "  'thumbnail': nan,\n",
       "  'retweet': False,\n",
       "  'nlikes': 9953,\n",
       "  'nreplies': 610,\n",
       "  'nretweets': 6896,\n",
       "  'quote_url': nan,\n",
       "  'search': 'None',\n",
       "  'near': nan,\n",
       "  'geo': nan,\n",
       "  'source': nan,\n",
       "  'user_rt_id': nan,\n",
       "  'user_rt': nan,\n",
       "  'retweet_id': nan,\n",
       "  'reply_to': '[]',\n",
       "  'retweet_date': nan,\n",
       "  'translate': nan,\n",
       "  'trans_src': nan,\n",
       "  'trans_dest': nan},\n",
       " {'Unnamed: 0': 1,\n",
       "  'id': 679145544673923072,\n",
       "  'conversation_id': 679145544673923072,\n",
       "  'created_at': 1450755890000.0,\n",
       "  'date': '2015-12-22 03:44:50',\n",
       "  'timezone': 0,\n",
       "  'place': nan,\n",
       "  'tweet': 'Live video from LZ-1  https://t.co/Ve6gEXfOdh',\n",
       "  'language': 'en',\n",
       "  'hashtags': '[]',\n",
       "  'cashtags': '[]',\n",
       "  'user_id': 44196397,\n",
       "  'user_id_str': 44196397,\n",
       "  'username': 'elonmusk',\n",
       "  'name': 'Elon Musk',\n",
       "  'day': 2,\n",
       "  'hour': 3,\n",
       "  'link': 'https://twitter.com/elonmusk/status/679145544673923072',\n",
       "  'urls': '[]',\n",
       "  'photos': '[]',\n",
       "  'video': 1,\n",
       "  'thumbnail': 'https://pbs.twimg.com/ext_tw_video_thumb/679145419507548161/pu/img/6DQ1zHR6pVDCJdeV.jpg',\n",
       "  'retweet': False,\n",
       "  'nlikes': 10710,\n",
       "  'nreplies': 373,\n",
       "  'nretweets': 5027,\n",
       "  'quote_url': nan,\n",
       "  'search': 'None',\n",
       "  'near': nan,\n",
       "  'geo': nan,\n",
       "  'source': nan,\n",
       "  'user_rt_id': nan,\n",
       "  'user_rt': nan,\n",
       "  'retweet_id': nan,\n",
       "  'reply_to': '[]',\n",
       "  'retweet_date': nan,\n",
       "  'translate': nan,\n",
       "  'trans_src': nan,\n",
       "  'trans_dest': nan}]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "elon_tweets[0:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd775d37-d9b6-457f-acf3-6cf561dbc27f",
   "metadata": {},
   "source": [
    "Responder los siguientes interrogantes de interés, para lo que se requiere:\n",
    "\n",
    "* Inspeccionar los datos\n",
    "* Formatearlos adecuadamente\n",
    "* Elegir y confeccionar nuevas estructuras de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9d7c9580-c873-4e53-aa64-5515982f9ee2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'2000': [0, 0, 0, '', '', ''], '2001': [0, 0, 0, '', '', ''], '2002': [0, 0, 0, '', '', ''], '2003': [0, 0, 0, '', '', ''], '2004': [0, 0, 0, '', '', ''], '2005': [0, 0, 0, '', '', ''], '2006': [0, 0, 0, '', '', ''], '2007': [0, 0, 0, '', '', ''], '2008': [0, 0, 0, '', '', ''], '2009': [0, 0, 0, '', '', ''], '2010': [0, 0, 0, '', '', ''], '2011': [0, 0, 0, '', '', ''], '2012': [0, 0, 0, '', '', ''], '2013': [0, 0, 0, '', '', ''], '2014': [0, 0, 0, '', '', ''], '2015': [0, 0, 0, '', '', ''], '2016': [0, 0, 0, '', '', ''], '2017': [0, 0, 0, '', '', ''], '2018': [0, 0, 0, '', '', ''], '2019': [0, 0, 0, '', '', ''], '2020': [0, 0, 0, '', '', ''], '2021': [0, 0, 0, '', '', ''], '2022': [0, 0, 0, '', '', '']}\n"
     ]
    }
   ],
   "source": [
    "dict_elon_tweets = {}\n",
    "\n",
    "for k in range(2000,2023):  # k es igual al rango de años de los tweets\n",
    "    k = str(k) \n",
    "    dict_elon_tweets[k] = [0,0,0,'','','']  # es una lista vacia donde irán los 3 tweets pas popolares \n",
    "    \n",
    "print(dict_elon_tweets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65b001ce-12b7-4b6a-b5e0-f067307d5c3a",
   "metadata": {
    "id": "65b001ce-12b7-4b6a-b5e0-f067307d5c3a"
   },
   "source": [
    "**1)** ¿Cuáles son los tweets (3) más populares *por año*? *(mayor cantidad de likes y retweets)*\n",
    "> Deben ser tweets y no retweets o respuestas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "05c2ec33-f4c4-45e7-87ea-bf6e1f699973",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Para cada año estos son los tres tweet con mayor popularidad {'2000': [0, 0, 0, '', '', ''], '2001': [0, 0, 0, '', '', ''], '2002': [0, 0, 0, '', '', ''], '2003': [0, 0, 0, '', '', ''], '2004': [0, 0, 0, '', '', ''], '2005': [0, 0, 0, '', '', ''], '2006': [0, 0, 0, '', '', ''], '2007': [0, 0, 0, '', '', ''], '2008': [0, 0, 0, '', '', ''], '2009': [0, 0, 0, '', '', ''], '2010': [5000, 0, 0, 'Please ignore prior tweets, as that was someone pretending to be me :)  This is actually me.', '', ''], '2011': [214387, 125469, 1614, \"Sew one button, doesn't make u a tailor; cook one meal, doesn't make u a chef; but f* one horse and u r a horsef*er for all of history...\", 'Kanye stopped by the SpaceX rocket factory today.  http://t.co/6z7gHBn6', 'Got called randomly by Kanye West today and received a download of his thoughts, ranging from shoes to Moses. He was polite, but opaque.'], '2012': [35239, 17765, 7460, 'Goal for Model S is to show that electric is way way better than gas. Combine w solar power &amp; the future looks bright.', 'Rough cut simulation of the SpaceX next gen rocket & spaceship design  http://t.co/cKBviQne', \"@DJjodes Would love to make a Tesla supertruck with crazy torque, dynamic air suspension and corners like its on rails. That'd be sweet...\"], '2013': [7880, 5598, 5345, 'Hyperloop Alpha at  http://t.co/ZRTcT2b8bP and  http://t.co/7cucKKprPB  http://t.co/LYhuRxUntA', 'Why does a Tesla fire w no injury get more media headlines than 100,000 gas car fires that kill 100s of people per year?', 'Forgot to say one thing at Tesla annual shareholders meeting: just as my money was the first in, it will be the last out.'], '2014': [22452, 10131, 9399, 'About time to unveil the D and something else  http://t.co/qp23yi59i6', 'Regarding Tesla patents  http://t.co/gGBWoInh6C', 'Noodles in Shinjuku  http://t.co/MWztTqByuA'], '2015': [62839, 26018, 25681, 'There and back again  https://t.co/Ll7wg2hL1G', '11 satellites deployed to target orbit and Falcon has landed back at Cape Canaveral. Headed to LZ-1. Welcome back, baby!', \"The rumor that I'm building a spaceship to get back to my home planet Mars is totally untrue\"], '2016': [50374, 45125, 44086, 'Traffic is driving me nuts. Am going to build a tunnel boring machine and just start digging...', 'Just heard that Norway will ban new sales of fuel cars in 2025. What an amazingly awesome country. You guys rock!!  https://t.co/uAXuBkDYuR', 'Forming a rocket nozzle  https://t.co/QrpcVyHAXr'], '2017': [691687, 521141, 368523, 'Am departing presidential councils. Climate change is real. Leaving Paris is not good for America or the world.', '0 to 100 km/h in 1.9 sec  https://t.co/xTOTDGuwQj', 'Nuclear alien UFO from North Korea  https://t.co/GUIHpKkkp5'], '2018': [1566588, 589093, 550725, 'Had to been done ur welcome  https://t.co/7jT0f9lqIS', 'View from SpaceX Launch Control. Apparently, there is a car in orbit around Earth.  https://t.co/QljN2VnL1O', 'i🖤anime'], '2019': [910374, 865557, 846785, 'If life is a video game, the graphics are great, but the plot is confusing &amp; the tutorial is way too long', 'And I am forever grateful  https://t.co/kU1pT8t0yv', 'Baby Yoda 💕  https://t.co/jpQvCccHb9'], '2020': [1899286, 986574, 888177, 'The coronavirus panic is dumb', ' https://t.co/e9dPKVSjjl', '5 mins to T-0'], '2021': [1159144, 1001812, 993661, 'ur welcome  https://t.co/e2KF57KLxb', 'My 14-year-old son, Saxon, said he feels like 2021 will be a good year. I agree. Let us all make it so.', 'Legalize comedy'], '2022': [0, 0, 0, '', '', '']}\n"
     ]
    }
   ],
   "source": [
    "for i in range(0,3): # una lista para agg los 3 mas populares\n",
    "    for dict_elon in elon_tweets:\n",
    "        is_retweet = dict_elon['retweet']\n",
    "        is_reply = dict_elon['reply_to']\n",
    "        if not is_retweet and is_reply:\n",
    "            years = dict_elon['date'][0:4] # año de los tweet\n",
    "            likes = int(dict_elon['nlikes'])  # n° de likes \n",
    "            retweet = int(dict_elon['nretweets']) # n° de retweet\n",
    "            tweet = dict_elon['tweet'] # se trae los string de los tweet\n",
    "            sum_likes_retweet = likes + retweet  \n",
    "            value = dict_elon_tweets[years][i]\n",
    "            if sum_likes_retweet > value and tweet not in dict_elon_tweets[years]:\n",
    "                dict_elon_tweets[years][i] = sum_likes_retweet\n",
    "                dict_elon_tweets[years][i+3] = tweet \n",
    "                \n",
    "print(f' Para cada año estos son los tres tweet con mayor popularidad {dict_elon_tweets}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "111cd58e-7c14-437c-9a58-d269504d3dc6",
   "metadata": {
    "id": "111cd58e-7c14-437c-9a58-d269504d3dc6"
   },
   "source": [
    "**2)** ¿Cuáles son las palabras más frecuentes en esos tweets? ¿Se puede inferir alguna conclusión sobre *los sentimientos* de dichos tweets?\n",
    "> Obviar las *stop_words*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bef35c63-34ac-4113-b85b-98f218116560",
   "metadata": {},
   "outputs": [],
   "source": [
    "en_stop_words = ['a', 'about', 'above', 'after', 'again', 'against', 'ain', 'all', 'am', 'an', 'and', 'any', 'are', 'aren', \"aren't\", 'as', 'at', 'be', 'because', \n",
    "                 'been', 'before', 'being', 'below', 'between', 'both', 'but', 'by', 'can', 'couldn', \"couldn't\", 'd', 'did', 'didn', \"didn't\", 'do', 'does', 'doesn', \n",
    "                 \"doesn't\", 'doing', 'don', \"don't\", 'down', 'during', 'each', 'few', 'for', 'from', 'further', 'had', 'hadn', \"hadn't\", 'has', 'hasn', \"hasn't\", \n",
    "                 'have', 'haven', \"haven't\", 'having', 'he', 'her', 'here', 'hers', 'herself', 'him', 'himself', 'his', 'how', 'i', 'if', 'in', 'into', 'is', 'isn', \n",
    "                 \"isn't\", 'it', \"it's\", 'its', 'itself', 'just', 'll', 'm', 'ma', 'me', 'mightn', \"mightn't\", 'more', 'most', 'mustn', \"mustn't\", 'my', 'myself', \n",
    "                 'needn', \"needn't\", 'no', 'nor', 'not', 'now', 'o', 'of', 'off', 'on', 'once', 'only', 'or', 'other', 'our', 'ours', 'ourselves', 'out', 'over', \n",
    "                 'own', 're', 's', 'same', 'shan', \"shan't\", 'she', \"she's\", 'should', \"should've\", 'shouldn', \"shouldn't\", 'so', 'some', 'such', 't', 'than', \n",
    "                 'that', \"that'll\", 'the', 'their', 'theirs', 'them', 'themselves', 'then', 'there', 'these', 'they', 'this', 'those', 'through', 'to', 'too', \n",
    "                 'under', 'until', 'up', 've', 'very', 'was', 'wasn', \"wasn't\", 'we', 'were', 'weren', \"weren't\", 'what', 'when', 'where', 'which', 'while', 'who', \n",
    "                 'whom', 'why', 'will', 'with', 'won', \"won't\", 'wouldn', \"wouldn't\", 'y', 'you', \"you'd\", \"you'll\", \"you're\", \"you've\", 'your', 'yours', 'yourself', \n",
    "                 'yourselves']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "17b7233c-3be7-4aa2-95be-fd814c0790f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " La palabras mas frecuente de los tres tweets de cada año entre ellos son ['one', 'make', 'Tesla'] \n"
     ]
    }
   ],
   "source": [
    "list_values  = list(dict_elon_tweets.values())\n",
    "# print(list_values)\n",
    "\n",
    "list_strings = [] \n",
    "\n",
    "for iteration_tweets in list_values:\n",
    "    str_tweets = iteration_tweets[3]\n",
    "    list_strings.append(str_tweets)\n",
    "    str_tweets = iteration_tweets[4]\n",
    "    list_strings.append(str_tweets)\n",
    "    str_tweets = iteration_tweets[5]\n",
    "    list_strings.append(str_tweets)\n",
    "    \n",
    "# print(list_strings)\n",
    "\n",
    "list_strings = ''.join(list_strings)\n",
    "list_strings = list_strings.split(' ')\n",
    "\n",
    "\n",
    "freqs = {}\n",
    "for words in list_strings:\n",
    "    count = freqs.get(words,0)\n",
    "    if words not in en_stop_words and words:\n",
    "        freqs[words] = count + 1 \n",
    "        \n",
    "# print(freqs)\n",
    "\n",
    "list_values = list(freqs.values())\n",
    "max_value = max(list_values)\n",
    "list_of_keys = freqs.keys()\n",
    "key_max = []\n",
    "for keys in list_of_keys: \n",
    "    if freqs[keys] == max_value:\n",
    "        key_max += [keys]\n",
    "    \n",
    "        \n",
    "print(f' La palabras mas frecuente de los tres tweets de cada año entre ellos son {key_max} ')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3eaeae9-19b6-4512-ba98-94f79613d3a5",
   "metadata": {
    "id": "f3eaeae9-19b6-4512-ba98-94f79613d3a5"
   },
   "source": [
    "**3)** ¿Cuáles son las fechas en las que se publicaron los tweets más populares? ¿Se puede inferir alguna correlación con la fecha (y/u hora) y su popularidad? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d7b3503e-5b53-4aa4-909f-19d5bcfac6fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'2000': [0, 0, 0, '', '', '', '', '', '', '', '', ''], '2001': [0, 0, 0, '', '', '', '', '', '', '', '', ''], '2002': [0, 0, 0, '', '', '', '', '', '', '', '', ''], '2003': [0, 0, 0, '', '', '', '', '', '', '', '', ''], '2004': [0, 0, 0, '', '', '', '', '', '', '', '', ''], '2005': [0, 0, 0, '', '', '', '', '', '', '', '', ''], '2006': [0, 0, 0, '', '', '', '', '', '', '', '', ''], '2007': [0, 0, 0, '', '', '', '', '', '', '', '', ''], '2008': [0, 0, 0, '', '', '', '', '', '', '', '', ''], '2009': [0, 0, 0, '', '', '', '', '', '', '', '', ''], '2010': [0, 0, 0, '', '', '', '', '', '', '', '', ''], '2011': [0, 0, 0, '', '', '', '', '', '', '', '', ''], '2012': [0, 0, 0, '', '', '', '', '', '', '', '', ''], '2013': [0, 0, 0, '', '', '', '', '', '', '', '', ''], '2014': [0, 0, 0, '', '', '', '', '', '', '', '', ''], '2015': [0, 0, 0, '', '', '', '', '', '', '', '', ''], '2016': [0, 0, 0, '', '', '', '', '', '', '', '', ''], '2017': [0, 0, 0, '', '', '', '', '', '', '', '', ''], '2018': [0, 0, 0, '', '', '', '', '', '', '', '', ''], '2019': [0, 0, 0, '', '', '', '', '', '', '', '', ''], '2020': [0, 0, 0, '', '', '', '', '', '', '', '', ''], '2021': [0, 0, 0, '', '', '', '', '', '', '', '', ''], '2022': [0, 0, 0, '', '', '', '', '', '', '', '', '']}\n"
     ]
    }
   ],
   "source": [
    "dict_elon_tweets = {}\n",
    "\n",
    "for k in range(2000,2023):  # k es igual al rango de años de los tweets\n",
    "    k = str(k) \n",
    "    dict_elon_tweets[k] = [0,0,0,'','','','','','','','','']  # es una lista vacia donde irán los 3 tweets pas popolares \n",
    "    \n",
    "print(dict_elon_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6294a3f2-e381-479a-94a3-703282d429eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Para cada año estas son las fechas de los tweets más populares {'2000': [0, 0, 0, '', '', '', '', '', '', '', '', ''], '2001': [0, 0, 0, '', '', '', '', '', '', '', '', ''], '2002': [0, 0, 0, '', '', '', '', '', '', '', '', ''], '2003': [0, 0, 0, '', '', '', '', '', '', '', '', ''], '2004': [0, 0, 0, '', '', '', '', '', '', '', '', ''], '2005': [0, 0, 0, '', '', '', '', '', '', '', '', ''], '2006': [0, 0, 0, '', '', '', '', '', '', '', '', ''], '2007': [0, 0, 0, '', '', '', '', '', '', '', '', ''], '2008': [0, 0, 0, '', '', '', '', '', '', '', '', ''], '2009': [0, 0, 0, '', '', '', '', '', '', '', '', ''], '2010': [5000, 0, 0, 'Please ignore prior tweets, as that was someone pretending to be me :)  This is actually me.', '', '', '06-04', '', '', 18, '', ''], '2011': [214387, 125469, 1614, \"Sew one button, doesn't make u a tailor; cook one meal, doesn't make u a chef; but f* one horse and u r a horsef*er for all of history...\", 'Kanye stopped by the SpaceX rocket factory today.  http://t.co/6z7gHBn6', 'Got called randomly by Kanye West today and received a download of his thoughts, ranging from shoes to Moses. He was polite, but opaque.', '12-30', '12-24', '12-21', 3, 1, 11], '2012': [35239, 17765, 7460, 'Goal for Model S is to show that electric is way way better than gas. Combine w solar power &amp; the future looks bright.', 'Rough cut simulation of the SpaceX next gen rocket & spaceship design  http://t.co/cKBviQne', \"@DJjodes Would love to make a Tesla supertruck with crazy torque, dynamic air suspension and corners like its on rails. That'd be sweet...\", '07-31', '01-24', '08-01', 21, 4, 2], '2013': [7880, 5598, 5345, 'Hyperloop Alpha at  http://t.co/ZRTcT2b8bP and  http://t.co/7cucKKprPB  http://t.co/LYhuRxUntA', 'Why does a Tesla fire w no injury get more media headlines than 100,000 gas car fires that kill 100s of people per year?', 'Forgot to say one thing at Tesla annual shareholders meeting: just as my money was the first in, it will be the last out.', '08-12', '11-19', '06-05', 20, 14, 2], '2014': [22452, 10131, 9399, 'About time to unveil the D and something else  http://t.co/qp23yi59i6', 'Regarding Tesla patents  http://t.co/gGBWoInh6C', 'Noodles in Shinjuku  http://t.co/MWztTqByuA', '10-02', '06-12', '09-07', 1, 17, 14], '2015': [62839, 26018, 25681, 'There and back again  https://t.co/Ll7wg2hL1G', '11 satellites deployed to target orbit and Falcon has landed back at Cape Canaveral. Headed to LZ-1. Welcome back, baby!', \"The rumor that I'm building a spaceship to get back to my home planet Mars is totally untrue\", '12-22', '12-22', '03-12', 3, 2, 22], '2016': [50374, 45125, 44086, 'Traffic is driving me nuts. Am going to build a tunnel boring machine and just start digging...', 'Just heard that Norway will ban new sales of fuel cars in 2025. What an amazingly awesome country. You guys rock!!  https://t.co/uAXuBkDYuR', 'Forming a rocket nozzle  https://t.co/QrpcVyHAXr', '12-17', '06-03', '12-09', 13, 18, 22], '2017': [691687, 521141, 368523, 'Am departing presidential councils. Climate change is real. Leaving Paris is not good for America or the world.', '0 to 100 km/h in 1.9 sec  https://t.co/xTOTDGuwQj', 'Nuclear alien UFO from North Korea  https://t.co/GUIHpKkkp5', '06-01', '11-17', '12-23', 20, 6, 2], '2018': [1566588, 589093, 550725, 'Had to been done ur welcome  https://t.co/7jT0f9lqIS', 'View from SpaceX Launch Control. Apparently, there is a car in orbit around Earth.  https://t.co/QljN2VnL1O', 'i🖤anime', '10-19', '02-06', '10-22', 21, 21, 22], '2019': [910374, 865557, 846785, 'If life is a video game, the graphics are great, but the plot is confusing &amp; the tutorial is way too long', 'And I am forever grateful  https://t.co/kU1pT8t0yv', 'Baby Yoda 💕  https://t.co/jpQvCccHb9', '12-12', '05-17', '12-13', 7, 6, 9], '2020': [1899286, 986574, 888177, 'The coronavirus panic is dumb', ' https://t.co/e9dPKVSjjl', '5 mins to T-0', '03-06', '06-26', '05-30', 20, 6, 19], '2021': [1159144, 1001812, 993661, 'ur welcome  https://t.co/e2KF57KLxb', 'My 14-year-old son, Saxon, said he feels like 2021 will be a good year. I agree. Let us all make it so.', 'Legalize comedy', '02-04', '01-09', '01-13', 7, 18, 9], '2022': [0, 0, 0, '', '', '', '', '', '', '', '', '']}\n"
     ]
    }
   ],
   "source": [
    "for i in range(0,3): # una lista para agg los 3 mas populares\n",
    "    for dict_elon in elon_tweets:\n",
    "        is_retweet = dict_elon['retweet']\n",
    "        is_reply = dict_elon['reply_to']\n",
    "        if not is_retweet and is_reply:\n",
    "            years = dict_elon ['date'][0:4] # año de los tweet\n",
    "            date = dict_elon ['date'][5:10] #fecha completa de los tweet\n",
    "            hour = dict_elon ['hour'] # hora de los tweets\n",
    "            likes = int(dict_elon['nlikes'])  # n° de likes \n",
    "            retweet = int(dict_elon['nretweets']) # n° de retweet\n",
    "            tweet = dict_elon['tweet'] # se trae los string de los tweet\n",
    "            sum_likes_retweet = likes + retweet  \n",
    "            value = dict_elon_tweets[years][i]\n",
    "            if sum_likes_retweet > value and tweet not in dict_elon_tweets[years]:\n",
    "                dict_elon_tweets[years][i] = sum_likes_retweet\n",
    "                dict_elon_tweets[years][i+3] = tweet \n",
    "                dict_elon_tweets[years][i+6] = date \n",
    "                dict_elon_tweets[years][i+9] = hour\n",
    "                \n",
    "              \n",
    "                \n",
    "print(f' Para cada año estas son las fechas de los tweets más populares {dict_elon_tweets}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a123911e-0328-4600-9a7f-0753e79bbd6d",
   "metadata": {
    "id": "a123911e-0328-4600-9a7f-0753e79bbd6d"
   },
   "source": [
    "**4)** ¿Cuáles son las personas (3) con las que más interactúa Elon?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7b3d9ba8-bdf0-4452-80e4-812d912cd54f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Estas son las tres personas ['Erdayastronaut', 'flcnhvy', 'PPathole'] con la que Elon Most más interactua\n"
     ]
    }
   ],
   "source": [
    "reply_to_list = []\n",
    "\n",
    "the_three_interacted = []\n",
    "\n",
    "for i in range (len(elon_tweets)):\n",
    "    try:\n",
    "        k = elon_tweets[i]['reply_to'].replace(\",\",\"\").replace(\"'\",\"\").split()[1]\n",
    "        reply_to_list.append(k)\n",
    "    except IndexError:\n",
    "        continue \n",
    "        \n",
    "most_interacted = max(set(reply_to_list), key = reply_to_list.count) \n",
    "the_three_interacted.append(most_interacted) # se agg la 1° persona con la que mas interactua \n",
    "# print(most_interacted) # se obtiene el nombre de la persona con la que Elon mas interactua\n",
    "\n",
    "reply_to_list_copy = reply_to_list.copy()\n",
    "second = (\" \".join(reply_to_list_copy)).replace(\"Erdayastronaut\",\"\").split() # se elimina la mayor interacción\n",
    "second_interacted = max(set(second), key = second.count) # se obtiene la 2° persona con la que mas interactua\n",
    "the_three_interacted.append(second_interacted) # agg el 2° con mayor interacción\n",
    "\n",
    "\n",
    "reply_to_list_copy1 = reply_to_list.copy()\n",
    "third = (\" \".join(reply_to_list_copy1)).replace(\"Erdayastronaut\",\"\").replace(\"flcnhvy\",\"\").split() # se elimina 1 y 2 mayores\n",
    "third_interacted = max(set(third), key = third.count) # se obtiene la tercera \n",
    "the_three_interacted.append(third_interacted) # agg el 3° mayor interaccion \n",
    "\n",
    "\n",
    "print(f' Estas son las tres personas {the_three_interacted} con la que Elon Most más interactua')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ff085be-3d8e-455b-a66b-c7e905f4c584",
   "metadata": {
    "id": "7ff085be-3d8e-455b-a66b-c7e905f4c584"
   },
   "source": [
    "**5)** ¿Cuáles son los tweets más populares con contenido multimedia? (Foto o video) ¿Se puede inferir algo sobre *por qué* son los más populares?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9bd994fb-ea3f-44e1-9b92-d2a76234d005",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'2010': [0, 0, 0, '', '', ''], '2011': [0, 0, 0, '', '', ''], '2012': [0, 0, 0, '', '', ''], '2013': [0, 0, 0, '', '', ''], '2014': [0, 0, 0, '', '', ''], '2015': [0, 0, 0, '', '', ''], '2016': [0, 0, 0, '', '', ''], '2017': [0, 0, 0, '', '', ''], '2018': [0, 0, 0, '', '', ''], '2019': [0, 0, 0, '', '', ''], '2020': [0, 0, 0, '', '', ''], '2021': [0, 0, 0, '', '', ''], '2022': [0, 0, 0, '', '', '']}\n"
     ]
    }
   ],
   "source": [
    "dict_tweets_photos = {}\n",
    "for k in range(2010,2023):  # k es igual al rango de años de los tweets\n",
    "    k = str(k) \n",
    "    dict_tweets_photos[k] = [0,0,0,'','','']  # es una lista vacia donde irán los tweets mas populares \n",
    "    \n",
    "print(dict_tweets_photos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e8c41569-5292-4fe6-b76a-1f2943377a3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Estos son los tweets con contenido de fotos o videos mas poupulares {'2010': [5000, 0, 0, 'Please ignore prior tweets, as that was someone pretending to be me :)  This is actually me.', '', ''], '2011': [214387, 125469, 1614, \"Sew one button, doesn't make u a tailor; cook one meal, doesn't make u a chef; but f* one horse and u r a horsef*er for all of history...\", 'Kanye stopped by the SpaceX rocket factory today.  http://t.co/6z7gHBn6', 'Got called randomly by Kanye West today and received a download of his thoughts, ranging from shoes to Moses. He was polite, but opaque.'], '2012': [35239, 17765, 7460, 'Goal for Model S is to show that electric is way way better than gas. Combine w solar power &amp; the future looks bright.', 'Rough cut simulation of the SpaceX next gen rocket & spaceship design  http://t.co/cKBviQne', \"@DJjodes Would love to make a Tesla supertruck with crazy torque, dynamic air suspension and corners like its on rails. That'd be sweet...\"], '2013': [7880, 5598, 5345, 'Hyperloop Alpha at  http://t.co/ZRTcT2b8bP and  http://t.co/7cucKKprPB  http://t.co/LYhuRxUntA', 'Why does a Tesla fire w no injury get more media headlines than 100,000 gas car fires that kill 100s of people per year?', 'Forgot to say one thing at Tesla annual shareholders meeting: just as my money was the first in, it will be the last out.'], '2014': [22452, 10131, 9399, 'About time to unveil the D and something else  http://t.co/qp23yi59i6', 'Regarding Tesla patents  http://t.co/gGBWoInh6C', 'Noodles in Shinjuku  http://t.co/MWztTqByuA'], '2015': [62839, 26018, 25681, 'There and back again  https://t.co/Ll7wg2hL1G', '11 satellites deployed to target orbit and Falcon has landed back at Cape Canaveral. Headed to LZ-1. Welcome back, baby!', \"The rumor that I'm building a spaceship to get back to my home planet Mars is totally untrue\"], '2016': [50374, 45125, 44086, 'Traffic is driving me nuts. Am going to build a tunnel boring machine and just start digging...', 'Just heard that Norway will ban new sales of fuel cars in 2025. What an amazingly awesome country. You guys rock!!  https://t.co/uAXuBkDYuR', 'Forming a rocket nozzle  https://t.co/QrpcVyHAXr'], '2017': [691687, 521141, 368523, 'Am departing presidential councils. Climate change is real. Leaving Paris is not good for America or the world.', '0 to 100 km/h in 1.9 sec  https://t.co/xTOTDGuwQj', 'Nuclear alien UFO from North Korea  https://t.co/GUIHpKkkp5'], '2018': [1566588, 589093, 550725, 'Had to been done ur welcome  https://t.co/7jT0f9lqIS', 'View from SpaceX Launch Control. Apparently, there is a car in orbit around Earth.  https://t.co/QljN2VnL1O', 'i🖤anime'], '2019': [910374, 865557, 846785, 'If life is a video game, the graphics are great, but the plot is confusing &amp; the tutorial is way too long', 'And I am forever grateful  https://t.co/kU1pT8t0yv', 'Baby Yoda 💕  https://t.co/jpQvCccHb9'], '2020': [1899286, 986574, 888177, 'The coronavirus panic is dumb', ' https://t.co/e9dPKVSjjl', '5 mins to T-0'], '2021': [1159144, 1001812, 993661, 'ur welcome  https://t.co/e2KF57KLxb', 'My 14-year-old son, Saxon, said he feels like 2021 will be a good year. I agree. Let us all make it so.', 'Legalize comedy'], '2022': [0, 0, 0, '', '', '']}\n"
     ]
    }
   ],
   "source": [
    "for i in range(0,3):\n",
    "    for dict_i in elon_tweets:\n",
    "        is_retweet = dict_i['retweet']\n",
    "        is_reply = dict_i['reply_to']\n",
    "        photos = dict_i['photos'] # trae la fotos del tweets \n",
    "        videos = dict_i['video'] # trae el video \n",
    "        if photos or videos: # si hay contenido de foto o video se ejecuta lo demás \n",
    "            if not is_retweet and is_reply: # que los tweets no sea retweet ni respuesta \n",
    "                likes = int(dict_i['nlikes'])  # n° de likes \n",
    "                retweet = int(dict_i['nretweets']) # n° de retweet\n",
    "                tweet = dict_i['tweet'] # se trae los tweets \n",
    "                sum_likes_retweet = likes + retweet # interacion de tweets y retweet\n",
    "                years = dict_i['date'][0:4] # año de los tweets\n",
    "                value = dict_tweets_photos[years][i] \n",
    "            if sum_likes_retweet > value and tweet not in dict_tweets_photos[years]:\n",
    "                dict_tweets_photos[years][i] = sum_likes_retweet \n",
    "                dict_tweets_photos[years][i+3] = tweet\n",
    "                \n",
    "print(f' Estos son los tweets con contenido de fotos o videos mas poupulares {dict_tweets_photos}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a8016d8-a00d-4bc4-ac5a-db4e197daae8",
   "metadata": {},
   "source": [
    "**6)** ¿Qué emojis se pueden encontrar en los tweets de Elon?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "94fba2b2-447e-47d4-9aaa-4678af61fc9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('en_emojis.pickle', 'rb') as handle:\n",
    "    en_emojis = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9023bc6d-4949-4a52-a29b-935b4b52fbb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# en_emojis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ea915ecf-a238-4cd1-b312-993c16275e13",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_en_emojis = []\n",
    "\n",
    "for key in en_emojis.keys():\n",
    "    list_en_emojis.append(key) \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e1859ac3-a95e-4f0a-96ec-df749b268e8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Los emojis que Elon usa en sus tweets son los siguientes {'👇', '🧛\\u200d♀️', '🧲', '🧱', '🐐', '🕊', '👾', '😛', '🧝\\u200d♂', '😱', '🚀', '🍟', '💝', '🧀', '🧝\\u200d♂️', '🌸', '👻', '☺️', '🤖', '🤡', '🙄', '💨', '💰', '😮', '🧙\\u200d♂️', '🧝', '🐣', '🌈', '🙃', '🪑', '😇', '😊', '👽', '🤹', '🌌', '🥇', '🌹', '💫', '🦶', '💵', '💛', '🍷', '😔', '🤘', '👩', '⛺', '🎩', '✨', '🛸', '🧡', '🍃', '🤣', '👨\\u200d🚀', '♠️', '⚔️', '🏴\\u200d☠️', '👨\\u200d🔬', '🌃', '🎷', '❤', '😈', '🦄', '♠', '♀️', '🤹🏻', '⛪', '🎤', '🐉', '🦙', '✌️', '🏴', '☠️', '👩\\u200d🚀', '💣', '😀', '🌏', '💙', '😞', '🧛', '🇩🇪', '🧦', '🦌', '🇯🇵', '🐱', '🎄', '🧚\\u200d♀', '🐻', '🇳🇱', '🍓', '👍', '💕', '🧙\\u200d♂', '⬇️', '☃️', '💚', '☺', '🔭', '🕺', '™️', '🎼', '🍕', '🏴\\u200d☠', '🧙', '🐈', '🍻', '🚘', '❤️', '😂', '☀️', '😢', '🇳🇴', '♀', '🍁', '😎', '💖', '😉', '😍', '🇨🇦', '🎶', '🧚', '🦊', '🤠', '🐍', '😴', '👀', '🧠', '🎥', '🎅', '🧚\\u200d♀️', '🤹🏻\\u200d♂️', '🔥', '😏', '🕳', '🍜', '™', '😅', '😬', '🤞', '🛰', '☃', '⬇', '🇮🇸', '🏝', '🇦🇪', '🍩', '🐝', '🍭', '👨', '😘', '🐜', '🎂', '🇫🇮', '🍄', '👸', '💗', '🇧🇴', '🇦🇺', '♥', '✌', '♂', '🇺🇸', '🙏', '🎉', '💡', '👶🏻', '🐏', '😃', '🍒', '♂️', '🌪', '😲', '🐇', '🖤', '😋', '🍀', '☠', '🎯', '😜', '🇸🇮', '🧁', '👟', '🎃', '🥳', '🧨', '🤗', '🧛\\u200d♀', '🤔', '🏻', '⚔', '💦', '⚡', '🚙', '💯', '⭐', '🔬', '🥰', '⚾', '♥️', '🎮', '☀', '💩', '🍆', '💄', '🍂', '🚗', '👌', '🧐', '🥧', '💜', '🎁', '😐', '👁', '🤝', '💘', '⛄', '🦆', '🤓', '🤹🏻\\u200d♂', '🥜', '👆', '🇺🇦', '🇮🇩', '🐶', '🐌', '🐼', '🐿', '👶'}\n"
     ]
    }
   ],
   "source": [
    "list_emojis_tweets = [] \n",
    "\n",
    "for dict_elon in elon_tweets:\n",
    "    tweet = dict_elon['tweet'] # se obtiene los tweets \n",
    "    # text_tweets = (\" \").join(tweet).split(\" \")\n",
    "    for emoji in list_en_emojis: # se guarda la lista de emojis en una vaiable auxiliar\n",
    "        if emoji in tweet: # se busca los emojis entre los tweets \n",
    "            list_emojis_tweets.append(emoji) # se agg los emojis de Elon a una lista nueva  \n",
    "        \n",
    "\n",
    "print(f' Los emojis que Elon usa en sus tweets son los siguientes {set(list_emojis_tweets)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00a3ad79-7389-4e03-aacb-70513df23df6",
   "metadata": {},
   "source": [
    "**7)** ¿Cuáles son los (3) más usados? ¿Se pueden inferir *sentimientos* a partir de los emojis más usados?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc62e49b-d585-4e18-856d-d2af2b7f277e",
   "metadata": {},
   "source": [
    "Si, durante el análisis de estos datos se encontró algo interesante, gracioso o que se quiera destacar, es posible hacerlo en el siguiente espacio:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ebf6fb0e-3663-4bd3-be08-e48cf871053f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_emojis_elon = {}\n",
    "for i in range (1,4):\n",
    "    k = i \n",
    "    dict_emojis_elon[k] = [\"\",0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7033338b-f003-4ea7-9f18-86427eec66e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: ['', 0], 2: ['', 0], 3: ['', 0]}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict_emojis_elon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "25bfa06d-e682-43b0-80a9-3917402124d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "frecuency = {}\n",
    "three_max_emojis = []\n",
    "\n",
    "for emojis_e in list_emojis_tweets:\n",
    "    count = frecuency.get(emojis_e,0)\n",
    "    if emojis_e in emojis_e:\n",
    "        frecuency[emojis_e] = count + 1  \n",
    "\n",
    "\n",
    "    \n",
    "first_emojis = max(frecuency.values())\n",
    "\n",
    "first_emojis_a = frecuency.get(391)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "4de276eb-ddfb-4f5b-9bac-60538aa81eda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " None\n"
     ]
    }
   ],
   "source": [
    "print(f' {first_emojis_a}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "bd109ab1-5851-4731-8e85-972db3ba3d95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'🔥': 81,\n",
       " '👌': 26,\n",
       " '💨': 55,\n",
       " '🐐': 8,\n",
       " '🎷': 4,\n",
       " '🐍': 7,\n",
       " '🎁': 13,\n",
       " '♥️': 207,\n",
       " '♥': 207,\n",
       " '💄': 2,\n",
       " '🤣': 391,\n",
       " '🤠': 2,\n",
       " '😬': 4,\n",
       " '😃': 8,\n",
       " '😏': 2,\n",
       " '💯': 16,\n",
       " '🖤': 99,\n",
       " '💵': 2,\n",
       " '👻': 20,\n",
       " '🎶': 20,\n",
       " '😉': 91,\n",
       " '💫': 46,\n",
       " '😀': 162,\n",
       " '🚀': 104,\n",
       " '👍': 57,\n",
       " '♠️': 2,\n",
       " '♠': 2,\n",
       " '🇨🇦': 6,\n",
       " '🤗': 39,\n",
       " '😴': 10,\n",
       " '👇': 2,\n",
       " '💰': 7,\n",
       " '🛰': 32,\n",
       " '😢': 20,\n",
       " '🤘': 2,\n",
       " '💩': 21,\n",
       " '⚡': 13,\n",
       " '🇩🇪': 7,\n",
       " '🍓': 2,\n",
       " '☺️': 23,\n",
       " '☺': 23,\n",
       " '👀': 11,\n",
       " '👆': 4,\n",
       " '🏝': 2,\n",
       " '😐': 5,\n",
       " '🎯': 2,\n",
       " '🇺🇸': 4,\n",
       " '🧙': 17,\n",
       " '♂️': 22,\n",
       " '♂': 22,\n",
       " '🧙\\u200d♂️': 17,\n",
       " '🧙\\u200d♂': 17,\n",
       " '😎': 17,\n",
       " '♀️': 16,\n",
       " '♀': 16,\n",
       " '🧛': 6,\n",
       " '🧛\\u200d♀️': 6,\n",
       " '🧛\\u200d♀': 6,\n",
       " '😮': 6,\n",
       " '💕': 50,\n",
       " '🕊': 2,\n",
       " '🌹': 8,\n",
       " '🇮🇩': 2,\n",
       " '🇧🇴': 2,\n",
       " '🐻': 18,\n",
       " '😈': 5,\n",
       " '🤔': 35,\n",
       " '✨': 6,\n",
       " '😂': 2,\n",
       " '🐈': 11,\n",
       " '😅': 22,\n",
       " '❤️': 36,\n",
       " '❤': 36,\n",
       " '🐝': 20,\n",
       " '🧚': 10,\n",
       " '👸': 2,\n",
       " '🧚\\u200d♀️': 10,\n",
       " '🧚\\u200d♀': 10,\n",
       " '🧲': 5,\n",
       " '🥜': 22,\n",
       " '🤓': 2,\n",
       " '🐇': 14,\n",
       " '💖': 9,\n",
       " '✌️': 17,\n",
       " '✌': 17,\n",
       " '🔭': 4,\n",
       " '😊': 10,\n",
       " '💘': 6,\n",
       " '🏻': 4,\n",
       " '🤹🏻\\u200d♂️': 2,\n",
       " '🤹🏻\\u200d♂': 2,\n",
       " '🤹': 2,\n",
       " '🤹🏻': 2,\n",
       " '🕺': 2,\n",
       " '☃️': 2,\n",
       " '☃': 2,\n",
       " '🪑': 2,\n",
       " '🍻': 2,\n",
       " '🌏': 13,\n",
       " '😞': 2,\n",
       " '👶': 3,\n",
       " '👶🏻': 2,\n",
       " '🚘': 49,\n",
       " '☀️': 2,\n",
       " '☀': 2,\n",
       " '🎥': 5,\n",
       " '🧱': 2,\n",
       " '🐌': 14,\n",
       " '🎄': 10,\n",
       " '🐼': 3,\n",
       " '🤞': 3,\n",
       " '⚔️': 3,\n",
       " '⚔': 3,\n",
       " '🧝': 3,\n",
       " '🧝\\u200d♂️': 3,\n",
       " '🧝\\u200d♂': 3,\n",
       " '💛': 14,\n",
       " '😘': 7,\n",
       " '🎃': 3,\n",
       " '🥇': 3,\n",
       " '™️': 9,\n",
       " '™': 9,\n",
       " '🍟': 3,\n",
       " '🧠': 25,\n",
       " '🕳': 3,\n",
       " '🎤': 3,\n",
       " '🦌': 3,\n",
       " '🦶': 3,\n",
       " '👁': 3,\n",
       " '🥧': 6,\n",
       " '💣': 3,\n",
       " '💦': 3,\n",
       " '🇮🇸': 6,\n",
       " '🇸🇮': 3,\n",
       " '🥳': 9,\n",
       " '😲': 9,\n",
       " '🇳🇱': 7,\n",
       " '🇳🇴': 19,\n",
       " '😋': 9,\n",
       " '🍀': 12,\n",
       " '🧐': 3,\n",
       " '🎩': 8,\n",
       " '🤝': 3,\n",
       " '🐏': 9,\n",
       " '🍷': 3,\n",
       " '👽': 31,\n",
       " '🤖': 10,\n",
       " '🦆': 9,\n",
       " '😇': 7,\n",
       " '🐣': 3,\n",
       " '🧨': 3,\n",
       " '⬇️': 3,\n",
       " '⬇': 3,\n",
       " '🌌': 7,\n",
       " '🏴': 3,\n",
       " '🏴\\u200d☠️': 3,\n",
       " '🏴\\u200d☠': 3,\n",
       " '☠️': 3,\n",
       " '☠': 3,\n",
       " '😔': 10,\n",
       " '🇫🇮': 3,\n",
       " '💙': 10,\n",
       " '🍭': 7,\n",
       " '🙏': 11,\n",
       " '😜': 13,\n",
       " '🍂': 3,\n",
       " '🍃': 10,\n",
       " '🍁': 3,\n",
       " '🇯🇵': 8,\n",
       " '💡': 10,\n",
       " '🧦': 6,\n",
       " '🧀': 3,\n",
       " '🧁': 3,\n",
       " '🐿': 3,\n",
       " '🦊': 7,\n",
       " '🐜': 6,\n",
       " '😍': 3,\n",
       " '👟': 3,\n",
       " '🤡': 6,\n",
       " '🎼': 7,\n",
       " '🥰': 3,\n",
       " '🌪': 3,\n",
       " '🐉': 7,\n",
       " '👨': 7,\n",
       " '👨\\u200d🚀': 3,\n",
       " '👩': 3,\n",
       " '👩\\u200d🚀': 3,\n",
       " '⭐': 3,\n",
       " '🇺🇦': 4,\n",
       " '🎅': 4,\n",
       " '💝': 4,\n",
       " '⛄': 4,\n",
       " '🍜': 4,\n",
       " '⚾': 4,\n",
       " '🦙': 4,\n",
       " '🛸': 16,\n",
       " '💗': 4,\n",
       " '👾': 8,\n",
       " '🎮': 4,\n",
       " '🐶': 4,\n",
       " '😛': 4,\n",
       " '🔬': 12,\n",
       " '👨\\u200d🔬': 4,\n",
       " '😱': 4,\n",
       " '🦄': 8,\n",
       " '🍩': 4,\n",
       " '🎂': 4,\n",
       " '🎉': 8,\n",
       " '🍒': 4,\n",
       " '💚': 4,\n",
       " '🧡': 4,\n",
       " '💜': 4,\n",
       " '🌈': 4,\n",
       " '⛺': 4,\n",
       " '⛪': 4,\n",
       " '🌃': 4,\n",
       " '🍄': 4,\n",
       " '🍕': 4,\n",
       " '🙃': 4,\n",
       " '🚗': 4,\n",
       " '🚙': 4,\n",
       " '🇦🇪': 4,\n",
       " '🌸': 4,\n",
       " '🐱': 4,\n",
       " '🇦🇺': 4,\n",
       " '🙄': 1,\n",
       " '🍆': 1}"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frecuency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4e178e0-0f54-4fee-bdb5-cbe6c87301df",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "analisis-de-producto.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
